\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage[colorinlistoftodos]{todonotes}

\usepackage{amsmath, amssymb, graphicx}
\usepackage{hyperref}

\title{Mediated Kalman Filter}
\author{Spencer Maughan}
\date{}

\begin{document}
	\maketitle

	\begin{abstract}
		This paper is intended to provide the motivation and derivation for accomplishing two goals. First, guaranteeing that underlying assumptions of a Kalman filter hold in practice. Second, providing a simplified tuning method such that a non-expert can comfortably ensure filter performance. The Kalman filter will be derived using Bayesian inference in hopes of setting the stage for the paradigm in which this method was developed. Following this, a case and method are presented for guaranteeing that the filter is mediated to maintain linear Gaussian assumptions. Finally, a proposal is made for simplified dynamic tuning of the Kalman filter.
	\end{abstract}

	\section{Introduction}
	There are two parts to the method outlined in this document.
	\begin{itemize}
		\item {\bfseries Mediation:} Maintaining confidence in the assumptions underlying the optimal performance of the kalman filter
		\item {\bfseries Simplified Tuning:} Estimating tuning parameters such that tuning is dramatically simplified and governed by reasonable noise estimates of the measurements used.
	\end{itemize}
Maintaining confidence in the assumptions held upon derivation of the Kalman filter has been a topic of discussion since its inception \cite{kalman1960new, maybeck1982stochastic}. This has usually taken the form of fault detection and implemented by use of the chi-square test \cite{jazwinski2007stochastic} or involved complex methods of handling that are unique to that scenario \cite{barfoot2017state}. The method outlined in this document varies from these traditional approaches in a more practical sense. The goal is not to just detect failure but prevent it. Making use of the chi-square test, a method of mediation will be provided such that a level of confidence in fundamental assumptions of the filter holds.

Adaptive tuning of Kalman filters is still an active area of research \cite{simon2006optimal, shumway2017time, sarkka2013bayesian}. Consequently, many self-tuning Kalman filters have been established. However, many of these approaches struggle as they attempt to be entirely hands-off. This method aims to simplify tuning and not eliminate it. It will be shown that by applying Bayesian inference, a conservative estimate of measurement noise can be dynamically provided. Also, a relationship between measurement and process noise will be established. The resulting tuning dimensions will be significantly reduced and made hopefully more intuitive to the non-expert.

Please note that this method can also be used in an extended Kalman filter for nonlinear cases.
	\section{Kalman Filter Derivation}
	This derivation follows notation used by Barfoot \cite{barfoot2017state}. Beginning with the Gaussian prior, our state is represented as:
	\begin{equation}
		p(x_{k-1} | \check{x}_{0}, \nu_{1:k-1}, y_{0:k-1}) \sim \mathcal{N}(\hat{x}_{k-1}, \hat{P}_{k-1})
	\end{equation}
	where $x_{k-1}$ is the true state and $\hat{x}_{k-1}$ is the mean estimate and $\hat{P}_{k-1}$ is the covariance matrix.

	\subsection{Prediction Step}
	We can derive the standard form of the prediction step simply using the latest input $\nu_k$ and generate the expression for the prior at time step $k$:
	\begin{equation}
	p(x_k | \check{x}_{0}, \nu_{1:k}, y_{0:k-1}) \sim \mathcal{N}(\check{x}_{k}, \check{P}_{k})
	\end{equation}
	where:
	\begin{align*}
		\check{x}_k &= E[A_{k-1}x_{k-1} + \nu_k + w_k] \\
		\check{x}_k &= A_{k-1} \underbrace{E[x_{k-1}]}_{\hat{x}_{k-1}} + \nu_k + \cancelto{0}{E[w_k]}
	\end{align*}

	\begin{equation}
		\check{x}_k = A_{k-1} \hat{x}_{k-1} + \nu_k
		\label{eq:process_model}
	\end{equation}

	and

	\begin{align*}
		\check{P}_k &= E[(x_{k} - E[x_k])(x_{k} - E[x_k])^T] \\
		 &= E[(A_{k-1}x_{k-1} + \nu_k + w_k - A_{k-1}\hat{x}_{k-1} - \nu_k)(A_{k-1}x_{k-1} + \nu_k + w_k - A_{k-1}\hat{x}_{k-1} - \nu_k)^T] \\
		 &= A_{k-1} \underbrace{E[(x_{k-1} - \hat{x}_{k-1})(x_{k-1} - \hat{x}_{k-1})^T]}_{\check{P}_{k-1}} A_{k-1}^T + \underbrace{E[w_kw_k^T]}_{Q_k} \\
	\end{align*}

	\begin{equation}
		\check{P}_k = A_{k-1} \hat{P}_{k-1} A_{k-1}^T + Q_k
		\label{eq:process_covariance}
	\end{equation}

	\subsection{Update Step Using Joint Density}
	The joint density of the state and latest measurement can be expressed as:
	\begin{align}
			p(x_k, y_k | \check{x}_{0}, \nu_{1:k}, y_{0:k-1}) & \sim \mathcal{N} \left(
		\begin{bmatrix}
			\mu_x \\
			\mu_y
		\end{bmatrix},
		\begin{bmatrix}
			\Sigma_{xx} & \Sigma_{xy} \\
			\Sigma_{yx} & \Sigma_{yy}
		\end{bmatrix} \right)\\
		& \sim \mathcal{N} \left(
		\begin{bmatrix}
			\check{x}_k \\
			C_k \check{x}_k
		\end{bmatrix},
		\begin{bmatrix}
			\check{P}_k & \check{P}_k C_k^T \\
			C_k \check{P}_k & C_k \check{P}_k C_k^T + R_k
		\end{bmatrix} \right)
	\end{align}
	Given this joint density, the conditional density can be directly written as follows:
	\begin{equation}
		p(x_k | \check{x}_{0}, \nu_{1:k}, y_{0:k}) \sim \mathcal{N} (\mu_x + \Sigma_{xy} \Sigma_{yy}^{-1} (y_k - \mu_y), \Sigma_{xx} -   \Sigma_{xy} \Sigma_{yy}^{-1} \Sigma_{yx})
	\end{equation}
	Leveraging this distribution we can provide the familiar structure of the Kalman filter update equations:
	\begin{align}
		K_k &=  \check{P}_k C_k^T (C_k \check{P}_k C_k^T + R_k)^{-1} \\
		\hat{x}_k &= \check{x}_k + K_k (y_k - C_k \check{x}_k) \\
		\hat{P}_k &= (I - K_k C_k) \check{P}_k
	\end{align}

	\section{Mediation}
	Mediation provides a rigorous framework for encapsulating and actively maintaining the fundamental assumptions of the Kalman filter—assumptions that are typically taken for granted. By continuously validating these assumptions, mediation enables the detection of both recoverable and non-recoverable failures, ensuring appropriate failure mode handling. This proactive approach enhances the practicality and reliability of the Kalman filter, making it more robust for real-world applications.


	A fundamental assumption of the Kalman filter is that both the actual measurement $y_k$ and the predicted measurement $C_k \check{x}_k$ share the same mean, which is a function of the true state $x_k$. Where both distributions are centered at $C_k x_k$, this can be expressed as:

	\begin{align}
		y_k &\sim \mathcal{N} (C_k x_k, R)\\
		C_k \check{x}_k &\sim \mathcal{N} (C_k x_k, C_k \check{P}_k C_k^T)\\
	\end{align}

	Explicitly testing the assumption that two random variables share the same mean can be formulated as a chi-squared test:

	\begin{equation}
		(y_k - C_k \check{x}_k)^T (C_k \check{P}_k C_k^T + R)^{-1} (y_k - C_k \check{x}_k) < \chi_c
		\label{eq:chisquare}
	\end{equation}
	where \( \chi_c \) is the critical value corresponding to a given confidence level of the chi-squared distribution, based on the degrees of freedom of the measurement. If this condition holds, the core assumptions of the filter remain valid, and no mediation is necessary. However, if the test fails, it indicates a fundamental breakdown of the filter’s underlying assumptions. At this point, the system is no longer operating within expected conditions, requiring appropriate corrective action to recover or transition to a safe failure mode.

	\subsection{Dynamic Tuning of Measurement Noise}
	As emphasized in the introduction, the goal of this method is not just to detect failures but to actively prevent them by reinforcing confidence that the fundamental assumptions of the Kalman filter hold in practice. One common source of assumption violations is unmodeled disturbances in sensor measurements, such as vibration, environmental noise, or mounting offsets. These disturbances introduce deviations that systematically increase the likelihood of failing the chi-squared test, thereby violating the core assumptions of the filter.

	A key strategy to mitigate this issue is dynamic estimation of measurement noise. Instead of relying on a fixed noise model that may not adapt to changing operational conditions, continuously updating the measurement noise covariance $R$ provides a more resilient and adaptive approach. By dynamically adjusting $R$ based on observed discrepancies between predicted and actual measurements, the filter becomes significantly more robust to external disturbances—reducing unnecessary failures while maintaining consistency in estimation.

	Leveraging measurement and expected measurement distributions we can derive the true Covariance matrix $R$:

	\begin{equation}
		R_n = {1 \over n - 1} \displaystyle\sum_{i=0} ^{n} (y_i - C_i x_i) (y_i - C_i x_i)^T
	\end{equation}

	In practical applications the true covariance is not available. A reliable alternative exists, given that the core assumptions of the filter are maintained. That is to use the current best estimates. In this is instance the expected value will result in a conservative estimate of covariance.

	\begin{align*}
		E[\hat{R}_k] &= E[{1 \over k - 1} \displaystyle\sum_{i=0} ^{k} (y_i - C_i \hat{x}_i) (y_i - C_i \hat{x}_i)^T]\\
		&= \underbrace{E[{1 \over k - 1} \displaystyle\sum_{i=0} ^{k} y_iy_i^T]}_{R_k} + \cancelto{0}{E[{1 \over k - 1} \displaystyle\sum_{i=0} ^{k} -C_i \hat{x}_i y_i - y_i C_i \hat{x}_i^T C_i^T]} + C_i \underbrace{E[{1 \over k - 1} \displaystyle\sum_{i=0}^{k} x_ix_i^T]}_{\hat{P}_k} C_i^T\\
	\end{align*}
	\begin{equation}
		E[\hat{R}_k] = R_k + C_k \hat{P}_k C_k^T
		\label{eq:exp_meas_cov}
	\end{equation}

	While the underlying assumptions of the Kalman filter are maintained this relationship provides a possibility of simplifying tuning of the filter. Taking this one step further recognizing the iterative nature of this algorithm we can dynamically calculate measurement covariance or noise and have convergence to the expected value in equation \ref{eq:exp_meas_cov}

	\subsubsection{Recursive Estimate of Covariance}

	\begin{equation}
		w_n = {1 \over n - 1} \displaystyle\sum_{i=0} ^{n} x_ix_i^T\\
	\end{equation}

	\begin{align*}
		w_{n+1} &= {1 \over n} \displaystyle\sum_{i=0} ^{n+1} x_ix_i^T\\
		&= {1 \over n} x_{n+1}x_{n+1}^T + {1 \over n} \displaystyle\sum_{i=0} ^{n} x_ix_i^T\\
		&= {1 \over n} x_{n+1}x_{n+1}^T + {n-1 \over n} \underbrace{{1 \over n -1} \displaystyle\sum_{i=0} ^{n} x_ix_i^T}_{w_n}\\
		&= {x_{n+1}x_{n+1}^T + (n - 1) w_n \over n}\\
	\end{align*}

	\begin{equation}
		w_{n+1} = = w_n + {x_{n+1}x_{n+1}^T -w_n \over n}
		\label{eq:recursive_covariance}
	\end{equation}

	\subsubsection{Dynamic Estimation of Measurement Noise}

	Applying equation \ref{eq:recursive_covariance} we can generate an expression for dynamic estimation of measurement noise which under our current assumptions will converge to the conservative estimated value demonstrated in equation \ref{eq:exp_meas_cov}.


	\begin{equation}
		\hat{R}_k = \hat{R}_{k-1} + {(y_k - C_k \hat{x}_k)(y_k - C_k \hat{x}_k)^T - \hat{R}_{k-1} \over n}
	\end{equation}

	Note that $(y_k - C_k \hat{x}_k)$ represents the innovation value and equation \ref{eq:exp_meas_cov} defines the innovation covariance.

	\subsection{Dynamic Tuning of Process Noise}

	Process noise is the Covariance of the error in the model at each time step:

	\begin{align*}
		\Delta \hat{x}_k &= \hat{x}_k - \hat{x}_{k-1}\\
		\Delta x_k &= x_k - x_{k-1}\\
		\Delta x_k^{err} &= \Delta \hat{x}_k - \Delta x_k
	\end{align*}

	\begin{equation}
		Q_n = {1 \over n - 1} \displaystyle\sum_{i=0} ^{n} \Delta x_i^{err} {\Delta x_i^{err}}^T
	\end{equation}

	Similar to our discussion of measurement covariance, in practical applications the true state is not available. However, we are actively assessing whether the core assumptions of the filter are maintained. While these hold a reasonable substitute for the true state is the updated state that incorporates measurement data. This estimate again will result in a conservative estimate of covariance. Given that both process and measurement noise are conservative estimates a single scalar tuning parameter $\zeta$ will be introduced to permit tuning the relative weighting between measurement and process.

	\begin{align*}
		Q_n &= {1 \over n - 1} \displaystyle\sum_{i=0} ^{n}(\hat{x}_k^- - \hat{x}_{k-1}^- - (\hat{x}_k^+ - \hat{x}_{k-1}^-))(\hat{x}_k^- - \hat{x}_{k-1}^- - (\hat{x}_k^+ - \hat{x}_{k-1}^-))^T\\
		&= {1 \over n - 1} \displaystyle\sum_{i=0} ^{n}(\hat{x}_k^- - \hat{x}_k^+)(\hat{x}_k^- - \hat{x}_k^+)^T\\
	\end{align*}
	Let $\hat{x}_k^-$ be the state predicted before the update step and let $\hat{x}_k^+$ be the state after the update step. Similarly applying equation \ref{eq:recursive_covariance} we have the following dynamic estimate of Q following the update step:

	\begin{equation}
	\hat{Q}_k = \hat{Q}_{k-1} + {\zeta (\hat{x}_k^- - \hat{x}_k^+)(\hat{x}_k^- - \hat{x}_k^+)^T - \hat{Q}_{k-1} \over n}
	\end{equation}

	\section{Complete Algorithm}

	\subsection{Tuning Parameters}
	Initializing parameters are needed. However, these do not need to be exposed at the interface level. Logical defaults typically suffice except for the initial state. The proper initialization of the state is not a topic of this paper.
	\begin{itemize}
		\item $Q_0 :=$ Initial Process noise
		\item $R_0 :=$ Initial Measurement noise per sensor
		\item $P_0 :=$ Initial State Covariance
		\item $\hat{x}_0 :=$ Initial State
	\end{itemize}

	Active Tuning parameters (all scalars):
		\begin{itemize}
		\item $\zeta :=$ Process noise modifier. Lower increases confidence in the process model and higher decreases confidence in process model. A value of $1.0$ is neutral. The value should be $\geq 0$.
		\item $n :=$ sample window over which adaptive tuning parameters will be estimated. The value should be an integer $\geq 0$.
		\item $\chi_c :=$ Acceptable threshold for a given sensor before mediation.
	\end{itemize}

	\subsection{Prediction Step}
	Using equations \ref{eq:process_model} and \ref{eq:process_covariance}:

	\begin{align*}
		\check{x}_k &= A_{k-1} \hat{x}_{k-1} + \nu_k\\
		\check{P}_k &= A_{k-1} \hat{P}_{k-1} A_{k-1}^T + Q_k
	\end{align*}

	\subsection{Mediation}
	Applying equation \ref{eq:chisquare}:

	If:
	\begin{equation*}
		(y_k - C_k \check{x}_k)^T (C_k \check{P}_k C_k^T + R)^{-1} (y_k - C_k \check{x}_k) < \chi_c
	\end{equation*}

	Then:
		\begin{center}
			Take corrective action to recover or transition to a safe failure mode.
		\end{center}

	Else:
		\begin{center}
			Continue
		\end{center}

	\subsection{Update Step}
	\begin{align*}
		K_k &=  \check{P}_k C_k^T (C_k \check{P}_k C_k^T + R_k)^{-1} \\
		\hat{x}_k &= \check{x}_k + K_k (y_k - C_k \check{x}_k) \\
		\hat{P}_k &= (I - K_k C_k) \check{P}_k
	\end{align*}

	\subsection{Measurement Noise Update}

	\begin{equation*}
		\hat{R}_k = \hat{R}_{k-1} + {(y_k - C_k \hat{x}_k)(y_k - C_k \hat{x}_k)^T - \hat{R}_{k-1} \over n}
	\end{equation*}

	\subsection{Process Noise Update}

	\begin{equation*}
		\hat{Q}_k = \hat{Q}_{k-1} + {\zeta (\hat{x}_k^- - \hat{x}_k^+)(\hat{x}_k^- - \hat{x}_k^+)^T - \hat{Q}_{k-1} \over n}
	\end{equation*}

\end{document}
